{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_name =  \"template\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# ploting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# models \n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 66666\n",
    "random.seed(myseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "_dataset_dir = \"./data\" # directory to get the data\n",
    "\n",
    "# training loop\n",
    "early_stop_steps = 300\n",
    "num_epochs = 10\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 1e-3\n",
    "weigth_decay = 1e-6\n",
    "\n",
    "# transforms for data augmentation\n",
    "image_size = 192\n",
    "\n",
    "# transform for testing set\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# transform for training set\n",
    "# more trasforms: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),   \n",
    "    transforms.RandomApply([   \n",
    "        transforms.RandomApply([transforms.CenterCrop((160,160))], p=0.4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomApply([transforms.RandomRotation(10, expand=False, center=None, fill=256)], p=0.35),\n",
    "        transforms.RandomApply([transforms.Pad(10, fill=256, padding_mode='constant')], p=0.3),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0, saturation=0, hue=0)], p=1),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0, contrast=0.4, saturation=0, hue=0)], p=0.3),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0, contrast=0, saturation=0.4, hue=0)], p=0.3),\n",
    "        ],\n",
    "    p=0.98),\n",
    "    transforms.Resize((image_size, image_size)),  \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "class for data loader in pytorch\n",
    "\n",
    "intit-> load the datas to a list\n",
    "\n",
    "getitem-> return an image and its label every time its called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path,tfm=test_tfm,files = None):\n",
    "        super(FaceDataset).__init__()\n",
    "        self.path = path\n",
    "\n",
    "        ### ===== get the files ==== ###\n",
    "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        ### ==== ====###\n",
    "        \n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        ### ==== return an image and its label ==== #\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "\n",
    "        # data augmentation\n",
    "        im = self.transform(im)\n",
    "        \n",
    "        # get the label for training\n",
    "        try:\n",
    "            label = int(fname.split(\"\\\\\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        #print(fname)\n",
    "        #print(fname.split(\"\\\\\")[-1].split(\"_\")[0])\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "nn.Conv2d-> CNN layer\n",
    "\n",
    "nn.BatchNorm2d -> batch normalization\n",
    "\n",
    "nn.ReLU -> activation function\n",
    "\n",
    "nn.MaxPool2d -> max pooling\n",
    "\n",
    "nn.Linear -> linerar layer\n",
    "\n",
    "forward() -> input an image and get the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input dimention [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128] 192\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64] 96\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32] 48\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16] 24\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8] 12\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4] 6 (img_size / (2 ** 5))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*6*6, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct datasets.\n",
    "# The argument \"loader\" tells how torchvision reads the data.\n",
    "train_set = FaceDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\n",
    "valid_set = FaceDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\n",
    "\n",
    "random.shuffle(valid_set.files)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trackers\n",
    "best_acc = 0\n",
    "best_loss = 0.0\n",
    "early_stop_cnt = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_acc_record = []\n",
    "val_acc_record = []\n",
    "\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "model = Classifier().to(device)\n",
    "\n",
    "# Use pytorch models \n",
    "# More pre-trained models: https://pytorch.org/vision/stable/models.html\n",
    "# model = models.vgg19_bn(pretrained=False, progress=False).to(device)\n",
    "# model = models.efficientnet_b0(pretrained=False, progress=False).to(device)\n",
    "\n",
    "# cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weigth_decay)\n",
    "\n",
    "# ================================ #\n",
    "# Haven't implememt early stop yet #\n",
    "# ================================ #\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "        print(f\"\\n--> starting epoch {epoch + 1}\")\n",
    "\n",
    "        ### ==== Training ==== ###\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # record information in training.\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "\n",
    "            # A batch consists of image data and corresponding labels.\n",
    "            imgs, labels = batch\n",
    "\n",
    "            # Put images through the model\n",
    "            outputs = model(imgs.to(device))\n",
    "\n",
    "            # Calculate the cross-entropy loss.\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the gradients for parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the gradient norms for stable training.\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "            # Update the parameters with computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (outputs.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record the loss and accuracy.\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "                \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "\n",
    "        # Print the information.\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc_record.append(train_acc)\n",
    "        print(f\"[ Train | [{epoch + 1:04d}/{num_epochs:04d}] ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "        ### ==== Validation ==== ###\n",
    "        # model eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # record information in validation.\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "\n",
    "        for batch in tqdm(valid_loader):\n",
    "\n",
    "            imgs, labels = batch\n",
    "\n",
    "\n",
    "            # torch.no_grad() accelerates the forward process.\n",
    "            with torch.no_grad():\n",
    "                outputs = model(imgs.to(device))\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            # Compute the accuracy\n",
    "            acc = (outputs.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record loss and accuracy.\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "\n",
    "        # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "\n",
    "        # Print the information.\n",
    "        val_losses.append(valid_loss)\n",
    "        val_acc_record.append(valid_acc)\n",
    "        print(f\"[ Valid | [{epoch + 1:04d}/{num_epochs:04d}] ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "        # we can use txt to record training process\n",
    "        \"\"\"\n",
    "        # update logs\n",
    "        if valid_acc > best_acc:\n",
    "            with open(f\"./{_exp_name}_log.txt\",\"a\") as log:\n",
    "                log.writelines(f\"[ Valid | [{epoch + 1:04d}/{num_epochs:04d}] ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\\n\")\n",
    "        else:\n",
    "            with open(f\"./{_exp_name}_log.txt\",\"a\") as log:\n",
    "                log.writelines(f\"[ Valid | [{epoch + 1:04d}/{num_epochs:04d}] ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\\n\")\n",
    "        \"\"\"\n",
    "        # save models\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"==== Best model found at epoch {epoch+1}, saving model ====\")\n",
    "            torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "            best_acc = valid_acc\n",
    "            best_loss = valid_loss\n",
    "            last_improved_cnt = 0\n",
    "            # with open('best.txt', 'w') as best:\n",
    "            #     best.write(f\"epoch = {epoch + 1}\\n\")\n",
    "            #     best.write(f\"train acc = {train_acc: .4f}\\n\")\n",
    "            #     best.write(f\"train loss = {train_loss: .4f}\\n\")\n",
    "            #     best.write(f\"best acc = {best_acc: .4f}\\n\")\n",
    "            #     best.write(f\"best loss = {best_loss: .4f}\\n\")\n",
    "        else:\n",
    "            early_stop_cnt += 1 \n",
    "        if last_improved_cnt >= early_stop_steps:\n",
    "            # early stop\n",
    "            print(f'early stop at epoch {epoch}')\n",
    "            break\n",
    "            \n",
    "print(f'\\n==> best acc: {best_acc: .4f} best loss : {best_loss: .4f}\\n')\n",
    "# plot the loss\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(np.squeeze(train_losses), label = \"train\")\n",
    "plt.plot(np.squeeze(val_losses), label = \"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# plot acc\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(np.squeeze(train_acc_record), label = \"train\")\n",
    "plt.plot(np.squeeze(val_acc_record), label = \"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = FaceDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model_best = Classifier().to(device)\n",
    "# model_best = models.vgg19_bn(pretrained=False, progress=False).to(device)\n",
    "\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for data,_ in test_loader:\n",
    "        test_pred = model_best(data.to(device))\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        prediction += test_label.squeeze().tolist()\n",
    "\n",
    "# verify corectness, save prdictions\n",
    "# def pad4(i):\n",
    "#     return \"0\"*(4-len(str(i)))+str(i)\n",
    "# df = pd.DataFrame()\n",
    "# df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n",
    "# df[\"Category\"] = prediction\n",
    "# df.to_csv(\"submission.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
